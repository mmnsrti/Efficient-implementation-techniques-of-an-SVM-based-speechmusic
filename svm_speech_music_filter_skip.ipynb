{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee071f3",
   "metadata": {},
   "source": [
    "# SVM-based Speech/Music Classifier + Efficient Implementation (Filtering & Skipping)\n",
    "\n",
    "**Paper implemented:** *Efficient implementation techniques of an SVM-based speech/music classifier in SMV* (Lim & Chang, 2015)\n",
    "\n",
    "این نوت‌بوک سه بخش اصلی را پیاده‌سازی می‌کند:\n",
    "\n",
    "1) **Baseline**: آموزش و ارزیابی یک **SVM از صفر با PyTorch (بدون sklearn)** برای تفکیک Speech vs Music روی فریم‌های صوتی  \n",
    "2) **Filtering (Hierarchical)**: یک **فیلتر ساده** که بخشی از فریم‌ها را بدون اجرای SVM به‌عنوان *music* تشخیص می‌دهد  \n",
    "3) **Skipping**: با تکیه بر **همبستگی بین فریم‌های متوالی**، اجرای SVM برای تعدادی از فریم‌های بعدی *skip* می‌شود  \n",
    "4) **Combined**: ترکیب Filtering → Skipping (همان ایده‌ی فلوچارت مقاله)\n",
    "\n",
    "> نکته مهم: در مقاله، ۶ ویژگی دقیقاً از داخل SMV codec استخراج می‌شوند.  \n",
    "> در این نوت‌بوک برای اینکه مستقل از SMV اجرا شود، نسخه‌ی **تقریبی/معادل** از آن ویژگی‌ها را از سیگنال خام پیاده‌سازی می‌کنیم.  \n",
    "> اگر شما دسترسی به ویژگی‌های داخلی SMV دارید، کافی است بخش `extract_features_for_frames` را با ویژگی‌های واقعی جایگزین کنید.\n",
    "\n",
    "---\n",
    "\n",
    "## پیش‌نیازها\n",
    "- فایل‌های صوتی Speech و Music (wav/mp3/…)\n",
    "- نصب کتابخانه‌ها: `numpy, scipy, torch, librosa, soundfile, matplotlib`\n",
    "\n",
    "ساختار پیشنهادی دیتا:\n",
    "```\n",
    "data/\n",
    "  speech/\n",
    "    s1.wav\n",
    "    s2.wav\n",
    "  music/\n",
    "    m1.wav\n",
    "    m2.wav\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d39cfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# اگر لازم بود (روی conda env شما)، این‌ها را نصب کنید:\n",
    "!pip -q install numpy scipy torch librosa soundfile matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4325a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e6b84",
   "metadata": {},
   "source": [
    "\n",
    "## 1) تنظیمات اصلی (مطابق مقاله تا حد امکان)\n",
    "- نرخ نمونه‌برداری: **8kHz**\n",
    "- طول فریم: **20ms**  → 160 نمونه\n",
    "- همپوشانی: اینجا 50% گذاشته شده (قابل تغییر)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ace39210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SR = 8000\n",
    "FRAME_MS = 20\n",
    "FRAME_LEN = int(SR * FRAME_MS / 1000)  # 160\n",
    "HOP_LEN = FRAME_LEN // 2               # 50% overlap\n",
    "\n",
    "# برچسب‌ها\n",
    "LABEL_SPEECH = 1\n",
    "LABEL_MUSIC = -1\n",
    "\n",
    "DATA_DIR = \"musan\"   # پوشه‌ی دیتا\n",
    "SPEECH_DIR = os.path.join(DATA_DIR, \"speech\")\n",
    "MUSIC_DIR  = os.path.join(DATA_DIR, \"music\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c6134",
   "metadata": {},
   "source": [
    "\n",
    "## 2) خواندن دیتا\n",
    "هر فایل به فریم‌ها تبدیل می‌شود؛ سپس هر فریم یک بردار ویژگی می‌گیرد.  \n",
    "برای جلوگیری از leakage، تقسیم train/test را **بر اساس فایل** انجام می‌دهیم (Group split).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a76dcb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech files: 426\n",
      "Music files : 660\n",
      "Example speech: ['musan/speech/librivox/speech-librivox-0000.wav', 'musan/speech/librivox/speech-librivox-0001.wav']\n",
      "Example music : ['musan/music/fma-western-art/music-fma-wa-0000.wav', 'musan/music/fma-western-art/music-fma-wa-0001.wav']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def list_audio_files(folder: str) -> List[str]:\n",
    "    exts = (\"*.wav\", \"*.flac\", \"*.mp3\", \"*.m4a\", \"*.ogg\")\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files.extend(glob.glob(os.path.join(folder, \"**\", e), recursive=True))\n",
    "    return sorted(files)\n",
    "\n",
    "speech_files = list_audio_files(SPEECH_DIR)\n",
    "music_files  = list_audio_files(MUSIC_DIR)\n",
    "\n",
    "print(\"Speech files:\", len(speech_files))\n",
    "print(\"Music files :\", len(music_files))\n",
    "print(\"Example speech:\", speech_files[:2])\n",
    "print(\"Example music :\", music_files[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e651816",
   "metadata": {},
   "source": [
    "\n",
    "## 3) استخراج ویژگی‌ها (نسخه‌ی نزدیک به مقاله، اما از سیگنال خام)\n",
    "\n",
    "مقاله از ۶ ویژگی استفاده می‌کند (ویژگی‌های داخلی SMV).  \n",
    "اینجا به صورت تقریبی می‌سازیم:\n",
    "\n",
    "- Energy: RMS\n",
    "- Reflection coeffs: با LPC و PARCOR (تقریبی)\n",
    "- Residual energy: انرژی residual از LPC\n",
    "- Pitch corr: بیشینه‌ی autocorr نرمال‌شده در بازه‌ی pitch (مثلاً 80–400Hz)\n",
    "- Periodicity counter: اگر pitch_corr از threshold گذشت، کانتر افزایش\n",
    "- Music continuity: کانتر ساده بر اساس (energy بالا + pitch_corr پایین‌تر) برای موسیقی\n",
    "\n",
    "> اگر شما ویژگی‌های واقعی SMV دارید، فقط همین سل را جایگزین کنید.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4348af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frame_signal(y: np.ndarray, frame_len: int, hop_len: int) -> np.ndarray:\n",
    "    # Returns frames: shape (num_frames, frame_len)\n",
    "    if len(y) < frame_len:\n",
    "        y = np.pad(y, (0, frame_len - len(y)))\n",
    "    n_frames = 1 + (len(y) - frame_len) // hop_len\n",
    "    frames = np.lib.stride_tricks.as_strided(\n",
    "        y,\n",
    "        shape=(n_frames, frame_len),\n",
    "        strides=(y.strides[0] * hop_len, y.strides[0]),\n",
    "        writeable=False,\n",
    "    )\n",
    "    return frames.copy()\n",
    "\n",
    "def levinson_durbin(r: np.ndarray, order: int):\n",
    "    # Levinson-Durbin recursion (returns LPC a, reflection k, and error e)\n",
    "    a = np.zeros(order + 1)\n",
    "    e = float(r[0])\n",
    "    k = np.zeros(order)\n",
    "\n",
    "    a[0] = 1.0\n",
    "    if e <= 1e-12:\n",
    "        return a, k, e\n",
    "\n",
    "    for i in range(1, order + 1):\n",
    "        acc = 0.0\n",
    "        for j in range(1, i):\n",
    "            acc += a[j] * r[i - j]\n",
    "        ki = (r[i] - acc) / (e + 1e-12)\n",
    "        k[i - 1] = ki\n",
    "\n",
    "        a_new = a.copy()\n",
    "        a_new[i] = ki\n",
    "        for j in range(1, i):\n",
    "            a_new[j] = a[j] - ki * a[i - j]\n",
    "        a = a_new\n",
    "\n",
    "        e *= (1.0 - ki * ki)\n",
    "        if e <= 1e-12:\n",
    "            break\n",
    "\n",
    "    return a, k, float(e)\n",
    "\n",
    "def autocorr_norm_peak(frame: np.ndarray, sr: int, fmin=80, fmax=400) -> float:\n",
    "    frame = frame - np.mean(frame)\n",
    "    denom = np.dot(frame, frame) + 1e-12\n",
    "    if denom <= 1e-12:\n",
    "        return 0.0\n",
    "    ac = np.correlate(frame, frame, mode=\"full\")\n",
    "    ac = ac[len(ac)//2:]  # non-negative lags\n",
    "    ac = ac / denom\n",
    "\n",
    "    lag_min = int(sr / fmax)\n",
    "    lag_max = int(sr / fmin)\n",
    "    lag_max = min(lag_max, len(ac) - 1)\n",
    "    if lag_max <= lag_min:\n",
    "        return 0.0\n",
    "\n",
    "    peak = np.max(ac[lag_min:lag_max+1])\n",
    "    return float(np.clip(peak, 0.0, 1.0))\n",
    "\n",
    "@dataclass\n",
    "class RunningState:\n",
    "    energy_ma: float = 0.0\n",
    "    refl_ma: float = 0.0\n",
    "    resid_ma: float = 0.0\n",
    "    pitch_ma: float = 0.0\n",
    "\n",
    "    cpr: int = 0\n",
    "    cpr_ma: float = 0.0\n",
    "    cpr_frame_count: int = 0\n",
    "\n",
    "    cM: float = 0.0\n",
    "    cM_ma: float = 0.0\n",
    "\n",
    "def extract_features_for_frames(\n",
    "    frames: np.ndarray, sr: int,\n",
    "    ema_alpha: float = 0.9,\n",
    "    lpc_order: int = 10,\n",
    "    periodicity_thr: float = 0.65,\n",
    "    cm_energy_thr: float = 0.02,\n",
    "    cm_pitch_thr: float = 0.45\n",
    ") -> np.ndarray:\n",
    "    # Outputs feature matrix (num_frames, 6): [energy_ma, refl_ma, resid_ma, pitch_ma, cpr_ma, cM_ma]\n",
    "    st = RunningState()\n",
    "    feats = []\n",
    "\n",
    "    for fr in frames:\n",
    "        rms = float(np.sqrt(np.mean(fr**2) + 1e-12))\n",
    "\n",
    "        fr_z = fr - np.mean(fr)\n",
    "        r = np.correlate(fr_z, fr_z, mode=\"full\")[len(fr_z)-1:]\n",
    "        r = r[:lpc_order+1]\n",
    "\n",
    "        a, k, e = levinson_durbin(r, lpc_order)\n",
    "        refl_summary = float(np.mean(np.abs(k))) if len(k) else 0.0\n",
    "        resid_energy = float(e / (r[0] + 1e-12)) if r[0] > 0 else 0.0\n",
    "\n",
    "        pitch_corr = autocorr_norm_peak(fr, sr)\n",
    "\n",
    "        st.energy_ma = ema_alpha * st.energy_ma + (1 - ema_alpha) * rms\n",
    "        st.refl_ma   = ema_alpha * st.refl_ma   + (1 - ema_alpha) * refl_summary\n",
    "        st.resid_ma  = ema_alpha * st.resid_ma  + (1 - ema_alpha) * resid_energy\n",
    "        st.pitch_ma  = ema_alpha * st.pitch_ma  + (1 - ema_alpha) * pitch_corr\n",
    "\n",
    "        periodicity_flag = 1 if pitch_corr >= periodicity_thr else 0\n",
    "        st.cpr += periodicity_flag\n",
    "        st.cpr_frame_count += 1\n",
    "        if st.cpr_frame_count >= 32:\n",
    "            st.cpr = 0\n",
    "            st.cpr_frame_count = 0\n",
    "\n",
    "        st.cpr_ma = ema_alpha * st.cpr_ma + (1 - ema_alpha) * st.cpr\n",
    "\n",
    "        # heuristic \"music continuity\"\n",
    "        if (rms >= cm_energy_thr) and (pitch_corr <= cm_pitch_thr):\n",
    "            st.cM = min(st.cM + 5, 400)\n",
    "        else:\n",
    "            st.cM = max(st.cM - 2, 0)\n",
    "\n",
    "        st.cM_ma = 0.9 * st.cM_ma + 0.1 * st.cM\n",
    "        feats.append([st.energy_ma, st.refl_ma, st.resid_ma, st.pitch_ma, st.cpr_ma, st.cM_ma])\n",
    "\n",
    "    return np.asarray(feats, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74702f9f",
   "metadata": {},
   "source": [
    "\n",
    "## 4) ساخت دیتاست فریم‌ها\n",
    "- هر فایل → (فریم‌ها، ویژگی‌ها)\n",
    "- برچسب هر فریم = برچسب فایل\n",
    "- `groups` = نام فایل (برای GroupSplit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_audio(path: str, sr: int) -> np.ndarray:\n",
    "    y, _sr = librosa.load(path, sr=sr, mono=True)\n",
    "    if np.max(np.abs(y)) > 0:\n",
    "        y = y / np.max(np.abs(y))\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def build_frame_dataset(files: List[str], label: int):\n",
    "    X_list, y_list, g_list = [], [], []\n",
    "    for p in files:\n",
    "        y = load_audio(p, SR)\n",
    "        frames = frame_signal(y, FRAME_LEN, HOP_LEN)\n",
    "        Xf = extract_features_for_frames(frames, SR)\n",
    "        X_list.append(Xf)\n",
    "        y_list.append(np.full((Xf.shape[0],), label, dtype=np.int32))\n",
    "        g_list.extend([os.path.basename(p)] * Xf.shape[0])\n",
    "    if not X_list:\n",
    "        return np.zeros((0,6), dtype=np.float32), np.zeros((0,), dtype=np.int32), []\n",
    "    return np.vstack(X_list), np.concatenate(y_list), g_list\n",
    "\n",
    "X_s, y_s, g_s = build_frame_dataset(speech_files, LABEL_SPEECH)\n",
    "X_m, y_m, g_m = build_frame_dataset(music_files,  LABEL_MUSIC)\n",
    "\n",
    "X = np.vstack([X_s, X_m]) if len(X_s) and len(X_m) else (X_s if len(X_s) else X_m)\n",
    "y = np.concatenate([y_s, y_m]) if len(y_s) and len(y_m) else (y_s if len(y_s) else y_m)\n",
    "groups = g_s + g_m\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y:\", y.shape, \"unique labels:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d424a7e",
   "metadata": {},
   "source": [
    "## 5) آموزش Baseline SVM از صفر (PyTorch)\n",
    "\n",
    "در مقاله از RBF استفاده شده است. اینجا هم از یک **SVM مبتنی بر hinge-loss** استفاده می‌کنیم که\n",
    "کرنل RBF را با **Random Fourier Features** تقریب می‌زند (بدون sklearn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a70c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utilities (no sklearn)\n",
    "# -----------------------------\n",
    "def group_train_test_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    groups: List[str],\n",
    "    test_size: float = 0.25,\n",
    "    random_state: int = 42,\n",
    "    max_tries: int = 200,\n",
    "):\n",
    "    g = np.asarray(groups)\n",
    "    unique_groups = np.unique(g)\n",
    "    if len(unique_groups) < 2:\n",
    "        raise RuntimeError(\"تعداد فایل‌ها برای group split کافی نیست.\")\n",
    "\n",
    "    n_test = int(round(test_size * len(unique_groups)))\n",
    "    n_test = max(1, min(len(unique_groups) - 1, n_test))\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    for _ in range(max_tries):\n",
    "        perm = rng.permutation(unique_groups)\n",
    "        test_groups = set(perm[:n_test].tolist())\n",
    "\n",
    "        test_mask = np.array([gi in test_groups for gi in g], dtype=bool)\n",
    "        train_idx = np.where(~test_mask)[0]\n",
    "        test_idx = np.where(test_mask)[0]\n",
    "\n",
    "        if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(np.unique(y[train_idx])) < 2:\n",
    "            continue\n",
    "        if len(np.unique(y[test_idx])) < 2:\n",
    "            continue\n",
    "\n",
    "        return train_idx, test_idx\n",
    "\n",
    "    raise RuntimeError(\"نتوانستم split متعادلی پیدا کنم. تعداد فایل‌های speech/music را بیشتر کنید.\")\n",
    "\n",
    "\n",
    "def accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_true.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.mean(y_true == y_pred))\n",
    "\n",
    "\n",
    "def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, labels: List[int]):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    cm = np.zeros((len(labels), len(labels)), dtype=np.int64)\n",
    "    for i, lt in enumerate(labels):\n",
    "        for j, lp in enumerate(labels):\n",
    "            cm[i, j] = int(np.sum((y_true == lt) & (y_pred == lp)))\n",
    "    return cm\n",
    "\n",
    "\n",
    "def classification_report(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    target_names: List[str],\n",
    "    labels: Optional[List[int]] = None,\n",
    ") -> str:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = sorted(np.unique(np.concatenate([y_true, y_pred])).tolist())\n",
    "\n",
    "    header = f\"{'class':<16}{'precision':>10}{'recall':>10}{'f1-score':>10}{'support':>10}\"\n",
    "    lines = [header, \"-\" * len(header)]\n",
    "\n",
    "    macro_p, macro_r, macro_f = 0.0, 0.0, 0.0\n",
    "    total_support = 0\n",
    "\n",
    "    for lbl, name in zip(labels, target_names):\n",
    "        tp = np.sum((y_true == lbl) & (y_pred == lbl))\n",
    "        fp = np.sum((y_true != lbl) & (y_pred == lbl))\n",
    "        fn = np.sum((y_true == lbl) & (y_pred != lbl))\n",
    "        support = int(np.sum(y_true == lbl))\n",
    "\n",
    "        p = float(tp / (tp + fp + 1e-12))\n",
    "        r = float(tp / (tp + fn + 1e-12))\n",
    "        f1 = float(2 * p * r / (p + r + 1e-12))\n",
    "\n",
    "        lines.append(f\"{name:<16}{p:>10.3f}{r:>10.3f}{f1:>10.3f}{support:>10d}\")\n",
    "\n",
    "        macro_p += p\n",
    "        macro_r += r\n",
    "        macro_f += f1\n",
    "        total_support += support\n",
    "\n",
    "    n_cls = max(len(labels), 1)\n",
    "    lines.append(\"-\" * len(header))\n",
    "    lines.append(f\"{'macro avg':<16}{macro_p/n_cls:>10.3f}{macro_r/n_cls:>10.3f}{macro_f/n_cls:>10.3f}{total_support:>10d}\")\n",
    "    lines.append(f\"{'accuracy':<16}{'':>10}{'':>10}{accuracy_score(y_true, y_pred):>10.3f}{total_support:>10d}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PyTorch SVM from scratch\n",
    "# -----------------------------\n",
    "class StandardScalerScratch:\n",
    "    def __init__(self, eps: float = 1e-8):\n",
    "        self.eps = eps\n",
    "        self.mean_ = None\n",
    "        self.scale_ = None\n",
    "\n",
    "    def fit(self, X: np.ndarray):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        self.mean_ = X.mean(axis=0, keepdims=True)\n",
    "        self.scale_ = X.std(axis=0, keepdims=True)\n",
    "        self.scale_[self.scale_ < self.eps] = 1.0\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        return (X - self.mean_) / self.scale_\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "\n",
    "class TorchRBFSVMScratch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma: float = 0.01,\n",
    "        C: float = 1.0,\n",
    "        n_rff: int = 512,\n",
    "        lr: float = 3e-3,\n",
    "        epochs: int = 25,\n",
    "        batch_size: int = 2048,\n",
    "        random_state: int = 42,\n",
    "        device: Optional[str] = None,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.C = C\n",
    "        self.n_rff = n_rff\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.rff_w_ = None\n",
    "        self.rff_b_ = None\n",
    "        self.rff_scale_ = np.sqrt(2.0 / self.n_rff)\n",
    "        self.w_ = None\n",
    "        self.b0_ = 0.0\n",
    "\n",
    "    def _rff_numpy(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.rff_scale_ * np.cos(X @ self.rff_w_ + self.rff_b_)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        labels = set(np.unique(y).tolist())\n",
    "        if not labels.issubset({-1.0, 1.0}):\n",
    "            raise ValueError(\"y باید فقط شامل -1 و +1 باشد.\")\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples == 0:\n",
    "            raise ValueError(\"X خالی است.\")\n",
    "\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.rff_w_ = rng.normal(\n",
    "            loc=0.0,\n",
    "            scale=np.sqrt(2.0 * self.gamma),\n",
    "            size=(n_features, self.n_rff),\n",
    "        ).astype(np.float32)\n",
    "        self.rff_b_ = rng.uniform(0.0, 2.0 * np.pi, size=(self.n_rff,)).astype(np.float32)\n",
    "\n",
    "        x_tensor = torch.from_numpy(X)\n",
    "        y_tensor = torch.from_numpy(y)\n",
    "        loader = DataLoader(\n",
    "            TensorDataset(x_tensor, y_tensor),\n",
    "            batch_size=min(self.batch_size, n_samples),\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        rff_w_t = torch.from_numpy(self.rff_w_).to(self.device)\n",
    "        rff_b_t = torch.from_numpy(self.rff_b_).to(self.device)\n",
    "\n",
    "        w = torch.zeros(self.n_rff, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "        b0 = torch.zeros(1, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "\n",
    "        opt = torch.optim.Adam([w, b0], lr=self.lr)\n",
    "\n",
    "        for ep in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(self.device)\n",
    "                yb = yb.to(self.device)\n",
    "\n",
    "                z = self.rff_scale_ * torch.cos(xb @ rff_w_t + rff_b_t)\n",
    "                scores = z @ w + b0\n",
    "\n",
    "                hinge = torch.clamp(1.0 - yb * scores, min=0.0)\n",
    "                loss = 0.5 * torch.sum(w * w) + self.C * torch.mean(hinge)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                total_loss += float(loss.detach().cpu())\n",
    "\n",
    "            if self.verbose and ((ep + 1) % 5 == 0 or ep == 0 or ep == self.epochs - 1):\n",
    "                print(f\"Epoch {ep+1:02d}/{self.epochs} - loss: {total_loss / len(loader):.5f}\")\n",
    "\n",
    "        self.w_ = w.detach().cpu().numpy().astype(np.float32)\n",
    "        self.b0_ = float(b0.detach().cpu().item())\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        z = self._rff_numpy(X)\n",
    "        return z @ self.w_ + self.b0_\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        return np.where(scores >= 0.0, LABEL_SPEECH, LABEL_MUSIC).astype(np.int32)\n",
    "\n",
    "\n",
    "class TorchSVMPipeline:\n",
    "    def __init__(self, **svm_kwargs):\n",
    "        self.scaler = StandardScalerScratch()\n",
    "        self.svm = TorchRBFSVMScratch(**svm_kwargs)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        Xs = self.scaler.fit_transform(X)\n",
    "        self.svm.fit(Xs, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        Xs = self.scaler.transform(X)\n",
    "        return self.svm.predict(Xs)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Evaluate baseline\n",
    "# -----------------------------\n",
    "if len(np.unique(y)) < 2 or len(y) < 10:\n",
    "    raise RuntimeError(f\"دیتای کافی ندارید. حداقل چند فایل speech و music داخل '{DATA_DIR}/' بگذارید.\")\n",
    "\n",
    "train_idx, test_idx = group_train_test_split(X, y, groups=groups, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "baseline = TorchSVMPipeline(\n",
    "    gamma=0.01,\n",
    "    C=1.0,\n",
    "    n_rff=512,\n",
    "    lr=3e-3,\n",
    "    epochs=25,\n",
    "    batch_size=2048,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "baseline.fit(X_train, y_train)\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "y_pred = baseline.predict(X_test)\n",
    "\n",
    "print(\"Train time (s):\", round(t1 - t0, 3))\n",
    "print(\"Baseline accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix [speech(+1), music(-1)]:\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=[LABEL_SPEECH, LABEL_MUSIC]))\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=[LABEL_SPEECH, LABEL_MUSIC],\n",
    "    target_names=[\"speech(+1)\", \"music(-1)\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3a6e1",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Filtering Mechanism (Hierarchical)\n",
    "\n",
    "اگر شرط ساده برقرار بود → music(-1) و SVM اجرا نمی‌شود.  \n",
    "در غیر این صورت → SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32239117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class FilterParams:\n",
    "    thr_cpr: float\n",
    "    thr_cM: float\n",
    "    op: str  # \"OR\" or \"AND\"\n",
    "\n",
    "def filter_predict_music_only(X_feat: np.ndarray, params: FilterParams) -> np.ndarray:\n",
    "    cpr = X_feat[:, 4]\n",
    "    cM  = X_feat[:, 5]\n",
    "    cond1 = cpr > params.thr_cpr\n",
    "    cond2 = cM  > params.thr_cM\n",
    "    op = params.op.upper()\n",
    "    if op == \"OR\":\n",
    "        return cond1 | cond2\n",
    "    if op == \"AND\":\n",
    "        return cond1 & cond2\n",
    "    raise ValueError(\"op باید OR یا AND باشد\")\n",
    "\n",
    "def hierarchical_predict(X_feat: np.ndarray, svm_model, params: FilterParams):\n",
    "    filt_mask = filter_predict_music_only(X_feat, params)\n",
    "    y_hat = np.zeros((X_feat.shape[0],), dtype=np.int32)\n",
    "    y_hat[filt_mask] = LABEL_MUSIC\n",
    "\n",
    "    idx = np.where(~filt_mask)[0]\n",
    "    if len(idx):\n",
    "        y_hat[idx] = svm_model.predict(X_feat[idx])\n",
    "\n",
    "    stats = {\n",
    "        \"filtered_ratio\": float(np.mean(filt_mask)),\n",
    "        \"svm_calls_ratio\": float(np.mean(~filt_mask)),\n",
    "    }\n",
    "    return y_hat, stats\n",
    "\n",
    "# مقدار شروع (برای دیتای شما شاید نیاز به tune داشته باشد)\n",
    "fparams = FilterParams(thr_cpr=2.0, thr_cM=150.0, op=\"OR\")\n",
    "\n",
    "y_h_filt, st_filt = hierarchical_predict(X_test, baseline, fparams)\n",
    "print(\"Hierarchical accuracy:\", accuracy_score(y_test, y_h_filt))\n",
    "print(\"Stats:\", st_filt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4571fbc5",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Skipping Mechanism (Inter-frame correlation)\n",
    "\n",
    "نسخه‌ی مبتنی بر **previous classifications** (ساده و نزدیک به مقاله).  \n",
    "Skipping روی یک stream معنی دارد، پس اینجا **برای هر فایل جدا** اجرا می‌کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class SkipParams:\n",
    "    N_prev: int = 4\n",
    "    thr_speech: int = 4\n",
    "    thr_music: int = 4\n",
    "    num_skip: int = 8\n",
    "\n",
    "def skipping_predict_for_sequence(X_seq: np.ndarray, svm_model, sp: SkipParams):\n",
    "    n = X_seq.shape[0]\n",
    "    y_hat = np.zeros((n,), dtype=np.int32)\n",
    "\n",
    "    history: List[int] = []\n",
    "    nskip = 0\n",
    "    cprev = None\n",
    "\n",
    "    svm_calls = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for t in range(n):\n",
    "        if nskip > 0 and cprev is not None:\n",
    "            y_hat[t] = cprev\n",
    "            nskip -= 1\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if len(history) >= sp.N_prev:\n",
    "            lastN = history[-sp.N_prev:]\n",
    "            speech_count = sum(1 for c in lastN if c == LABEL_SPEECH)\n",
    "            music_count  = sum(1 for c in lastN if c == LABEL_MUSIC)\n",
    "\n",
    "            if speech_count >= sp.thr_speech:\n",
    "                cprev = LABEL_SPEECH\n",
    "                nskip = sp.num_skip\n",
    "            elif music_count >= sp.thr_music:\n",
    "                cprev = LABEL_MUSIC\n",
    "                nskip = sp.num_skip\n",
    "\n",
    "        y_hat[t] = svm_model.predict(X_seq[t:t+1])[0]\n",
    "        svm_calls += 1\n",
    "        history.append(int(y_hat[t]))\n",
    "\n",
    "    stats = {\"skipped_ratio\": skipped / n, \"svm_calls_ratio\": svm_calls / n}\n",
    "    return y_hat, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04408c33",
   "metadata": {},
   "source": [
    "\n",
    "### اجرای skipping روی تست (file-by-file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_on_files_with_skipping(files: List[str], label: int, svm_model, sp: SkipParams):\n",
    "    y_all, yhat_all, stats_list = [], [], []\n",
    "    for p in files:\n",
    "        ysig = load_audio(p, SR)\n",
    "        frames = frame_signal(ysig, FRAME_LEN, HOP_LEN)\n",
    "        Xf = extract_features_for_frames(frames, SR)\n",
    "        y_true = np.full((Xf.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "        y_hat, st = skipping_predict_for_sequence(Xf, svm_model, sp)\n",
    "\n",
    "        y_all.append(y_true)\n",
    "        yhat_all.append(y_hat)\n",
    "        stats_list.append(st)\n",
    "\n",
    "    if not y_all:\n",
    "        return np.array([], dtype=np.int32), np.array([], dtype=np.int32), {}\n",
    "\n",
    "    y_all = np.concatenate(y_all)\n",
    "    yhat_all = np.concatenate(yhat_all)\n",
    "    stats = {\n",
    "        \"avg_skipped_ratio\": float(np.mean([s[\"skipped_ratio\"] for s in stats_list])),\n",
    "        \"avg_svm_calls_ratio\": float(np.mean([s[\"svm_calls_ratio\"] for s in stats_list])),\n",
    "    }\n",
    "    return y_all, yhat_all, stats\n",
    "\n",
    "sp = SkipParams(N_prev=4, thr_speech=4, thr_music=4, num_skip=8)\n",
    "\n",
    "test_groups = set([groups[i] for i in test_idx])\n",
    "speech_test_files = [p for p in speech_files if os.path.basename(p) in test_groups]\n",
    "music_test_files  = [p for p in music_files  if os.path.basename(p) in test_groups]\n",
    "\n",
    "y_ts, yhat_ts, st_s = predict_on_files_with_skipping(speech_test_files, LABEL_SPEECH, baseline, sp)\n",
    "y_tm, yhat_tm, st_m = predict_on_files_with_skipping(music_test_files,  LABEL_MUSIC,  baseline, sp)\n",
    "\n",
    "y_true_skip = np.concatenate([y_ts, y_tm]) if len(y_ts) and len(y_tm) else (y_ts if len(y_ts) else y_tm)\n",
    "y_pred_skip = np.concatenate([yhat_ts, yhat_tm]) if len(yhat_ts) and len(yhat_tm) else (yhat_ts if len(yhat_ts) else yhat_tm)\n",
    "\n",
    "print(\"Skipping accuracy:\", accuracy_score(y_true_skip, y_pred_skip))\n",
    "print(\"Skipping stats (speech):\", st_s)\n",
    "print(\"Skipping stats (music) :\", st_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474901d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Combined (Filtering → Skipping)\n",
    "\n",
    "Filtering اول اجرا می‌شود؛ اگر فریم توسط فیلتر به عنوان music تشخیص داده شد، SVM اجرا نمی‌شود.  \n",
    "در غیر این صورت، اگر skipping فعال باشد، SVM هم اجرا نمی‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class CombinedParams:\n",
    "    filt: FilterParams\n",
    "    skip: SkipParams\n",
    "\n",
    "def combined_predict_for_sequence(X_seq: np.ndarray, svm_model, cp: CombinedParams):\n",
    "    n = X_seq.shape[0]\n",
    "    y_hat = np.zeros((n,), dtype=np.int32)\n",
    "\n",
    "    history: List[int] = []\n",
    "    nskip = 0\n",
    "    cprev = None\n",
    "\n",
    "    svm_calls = 0\n",
    "    filtered = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for t in range(n):\n",
    "        filt_now = filter_predict_music_only(X_seq[t:t+1], cp.filt)[0]\n",
    "        if filt_now:\n",
    "            y_hat[t] = LABEL_MUSIC\n",
    "            filtered += 1\n",
    "            if nskip > 0:\n",
    "                if cprev == LABEL_MUSIC:\n",
    "                    nskip = max(nskip - 1, 0)\n",
    "                else:\n",
    "                    nskip = 0\n",
    "                    cprev = None\n",
    "            continue\n",
    "\n",
    "        if nskip > 0 and cprev is not None:\n",
    "            y_hat[t] = cprev\n",
    "            nskip -= 1\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if len(history) >= cp.skip.N_prev:\n",
    "            lastN = history[-cp.skip.N_prev:]\n",
    "            speech_count = sum(1 for c in lastN if c == LABEL_SPEECH)\n",
    "            music_count  = sum(1 for c in lastN if c == LABEL_MUSIC)\n",
    "\n",
    "            if speech_count >= cp.skip.thr_speech:\n",
    "                cprev = LABEL_SPEECH\n",
    "                nskip = cp.skip.num_skip\n",
    "            elif music_count >= cp.skip.thr_music:\n",
    "                cprev = LABEL_MUSIC\n",
    "                nskip = cp.skip.num_skip\n",
    "\n",
    "        y_hat[t] = svm_model.predict(X_seq[t:t+1])[0]\n",
    "        svm_calls += 1\n",
    "        history.append(int(y_hat[t]))\n",
    "\n",
    "    stats = {\n",
    "        \"filtered_ratio\": filtered / n,\n",
    "        \"skipped_ratio\": skipped / n,\n",
    "        \"svm_calls_ratio\": svm_calls / n,\n",
    "        \"total_saved_ratio\": (filtered + skipped) / n,\n",
    "    }\n",
    "    return y_hat, stats\n",
    "\n",
    "def predict_on_files_combined(files: List[str], label: int, svm_model, cp: CombinedParams):\n",
    "    y_all, yhat_all, stats_list = [], [], []\n",
    "    for p in files:\n",
    "        ysig = load_audio(p, SR)\n",
    "        frames = frame_signal(ysig, FRAME_LEN, HOP_LEN)\n",
    "        Xf = extract_features_for_frames(frames, SR)\n",
    "        y_true = np.full((Xf.shape[0],), label, dtype=np.int32)\n",
    "\n",
    "        y_hat, st = combined_predict_for_sequence(Xf, svm_model, cp)\n",
    "        y_all.append(y_true)\n",
    "        yhat_all.append(y_hat)\n",
    "        stats_list.append(st)\n",
    "\n",
    "    if not y_all:\n",
    "        return np.array([], dtype=np.int32), np.array([], dtype=np.int32), {}\n",
    "\n",
    "    y_all = np.concatenate(y_all)\n",
    "    yhat_all = np.concatenate(yhat_all)\n",
    "    keys = stats_list[0].keys()\n",
    "    stats = {k: float(np.mean([s[k] for s in stats_list])) for k in keys}\n",
    "    return y_all, yhat_all, stats\n",
    "\n",
    "cp = CombinedParams(\n",
    "    filt=FilterParams(thr_cpr=2.0, thr_cM=150.0, op=\"OR\"),\n",
    "    skip=SkipParams(N_prev=4, thr_speech=4, thr_music=4, num_skip=8),\n",
    ")\n",
    "\n",
    "y_cs, yhat_cs, st_cs = predict_on_files_combined(speech_test_files, LABEL_SPEECH, baseline, cp)\n",
    "y_cm, yhat_cm, st_cm = predict_on_files_combined(music_test_files,  LABEL_MUSIC,  baseline, cp)\n",
    "\n",
    "y_true_c = np.concatenate([y_cs, y_cm]) if len(y_cs) and len(y_cm) else (y_cs if len(y_cs) else y_cm)\n",
    "y_pred_c = np.concatenate([yhat_cs, yhat_cm]) if len(yhat_cs) and len(yhat_cm) else (yhat_cs if len(yhat_cs) else yhat_cm)\n",
    "\n",
    "print(\"Combined accuracy:\", accuracy_score(y_true_c, y_pred_c))\n",
    "print(\"Combined stats (speech):\", st_cs)\n",
    "print(\"Combined stats (music) :\", st_cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a8e1e",
   "metadata": {},
   "source": [
    "\n",
    "## 9) جمع‌بندی سریع\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149bc741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize(name: str, y_true: np.ndarray, y_pred: np.ndarray, extra: Optional[dict]=None):\n",
    "    print(\"=\"*80)\n",
    "    print(name)\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Confusion matrix [speech(+1), music(-1)]:\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[LABEL_SPEECH, LABEL_MUSIC]))\n",
    "    if extra:\n",
    "        print(\"Stats:\", extra)\n",
    "\n",
    "summarize(\"Baseline (frame-level)\", y_test, y_pred)\n",
    "summarize(\"Filtering-only (hierarchical)\", y_test, y_h_filt, st_filt)\n",
    "summarize(\"Skipping-only (file-by-file)\", y_true_skip, y_pred_skip, {\"speech\": st_s, \"music\": st_m})\n",
    "summarize(\"Combined (Filtering→Skipping, file-by-file)\", y_true_c, y_pred_c, {\"speech\": st_cs, \"music\": st_cm})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550485c5",
   "metadata": {},
   "source": [
    "\n",
    "## 10) (اختیاری) تیون کردن thresholdها برای Filtering\n",
    "چون مقیاس کانترها در نسخه‌ی تقریبی با مقاله یکسان نیست، این grid-search ساده کمک می‌کند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_search_filter(X_eval, y_eval, svm_model,\n",
    "                       thr_cpr_list, thr_cM_list, ops=(\"OR\",\"AND\"),\n",
    "                       min_acc: float = 0.98):\n",
    "    best = None\n",
    "    for op in ops:\n",
    "        for a in thr_cpr_list:\n",
    "            for b in thr_cM_list:\n",
    "                fp = FilterParams(thr_cpr=a, thr_cM=b, op=op)\n",
    "                yhat, st = hierarchical_predict(X_eval, svm_model, fp)\n",
    "                acc = accuracy_score(y_eval, yhat)\n",
    "                if acc >= min_acc:\n",
    "                    score = st[\"filtered_ratio\"]\n",
    "                    if best is None or score > best[\"filtered_ratio\"]:\n",
    "                        best = {\"params\": fp, \"acc\": acc, **st}\n",
    "    return best\n",
    "\n",
    "thr_cpr_list = [0.5, 1.0, 2.0, 3.0, 4.0]\n",
    "thr_cM_list  = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "best = grid_search_filter(X_test, y_test, baseline, thr_cpr_list, thr_cM_list, min_acc=0.98)\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a47a8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Done ✅\n",
    "\n",
    "برای نزدیک‌تر شدن به مقاله:\n",
    "1) ویژگی‌های دقیق SMV را از codec استخراج کن و جایگزین `extract_features_for_frames` کن  \n",
    "2) thresholdها و پارامترهای skipping را روی dataset خودت tune کن  \n",
    "3) معیار سرعت را با شمارش `svm_calls_ratio` و زمان اجرا بسنج  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
